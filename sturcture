Logic we used (end-to-end):

Webcam → face box
perception/camera.py reads frames.
perception/detector.py finds the face (MediaPipe; Haar fallback).
perception/stream.py crops the face region.
Face crop → 7 emotions (your trained ML)
perception/openface_classifier.py loads the OpenFace multitask backbone (weights/MTL_backbone.pth) and extracts features (emotion+AU).
Your trained classifier models/openface_emotion_clf.pkl maps those features to the 7 classes:
angry, disgust, fear, happy, neutral, sad, surprise.
Training/evaluation was done with perception/train_openface_classifier.py (it prints val accuracy + confusion matrix).
7 emotions → engagement bucket (smoothing + thresholds)
perception/engagement.py takes the emotion probabilities and outputs a bucket:
low if top emotion is in {angry, disgust, fear, sad} and top_prob ≥ low-threshold
high if top emotion is in {happy, surprise} and top_prob ≥ high-threshold
otherwise medium (this is where neutral usually falls)
It uses:
EMA smoothing (--alpha)
a persistence guard (--guard-seconds) so bucket must be stable before switching
Output JSON each frame includes: bucket, top_emotion, confidence, and timestamp.
Bucket → Furhat adaptive tutor behavior
integration/bucket_to_furhat_realtime.py connects to ws://localhost:9000/v1/events.
After each spoken question it:
clears old events
waits ~2s (--observe-seconds) and takes a majority bucket (with --min-majority)
high: immediately advance to next question
medium: wait 3s (--medium-delay-seconds) then advance
low: repeat the same question + give the answer again
It uses --wait-speak-end so Furhat doesn’t interrupt itself.
That’s the full logic chain we implemented.